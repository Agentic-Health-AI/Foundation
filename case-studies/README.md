# Case Studies: Proof of Work

The power of Agentic Health lies in what it does in the real world. This section collects anonymized stories of AI agents acting as tireless medical analysts, discovering insights, bridging gaps, and extending lives.

Want to share your story or workflow? Please read [CONTRIBUTING.md](../CONTRIBUTING.md) to ensure all data is anonymized before submitting a PR.

---

## Benchmarks & Evaluation Resources

These benchmarks and evaluation frameworks can be used to validate agentic health workflows and measure AI performance in clinical settings.

- **[AgentClinic](https://github.com/SamuelSchmidgall/AgentClinic)**: First open-source benchmark for evaluating LLM agents in simulated clinical environments, supporting 24 cognitive biases in doctor and patient agents.

- **[MedAgentBench](https://github.com/stanfordmlgroup/MedAgentBench)**: Stanford benchmark providing a realistic virtual EHR environment to test medical LLM agent capabilities.

- **[Open Medical-LLM Leaderboard](https://huggingface.co/blog/leaderboard-medicalllm)**: HuggingFace leaderboard benchmarking LLMs across standardized medical exams and clinical reasoning tasks.

- **[MedLLMs Practical Guide](https://github.com/AI-in-Health/MedLLMsPracticalGuide)**: Published in Nature Reviews Bioengineering, providing comparison tables and curated resources for evaluating medical LLMs.
