# Safety & Benchmarks

Guardrails, evaluation frameworks, and benchmarks for medical AI safety.

- **[AgentClinic](https://github.com/SamuelSchmidgall/AgentClinic)**: First open benchmark for LLM agents in clinical environments, providing standardized evaluation metrics for agent behavior in healthcare settings.

- **[MedAgentBench](https://github.com/stanfordmlgroup/MedAgentBench)**: Stanford's EHR-focused agent evaluation framework designed to assess medical agents' ability to interact with electronic health records safely and effectively.

- **[NVIDIA NeMo Guardrails](https://github.com/NVIDIA/NeMo-Guardrails)**: Programmable guardrails framework with healthcare RAG examples, enabling developers to define and enforce safety constraints for medical AI systems.

- **[Open Medical-LLM Leaderboard](https://huggingface.co/blog/leaderboard-medicalllm)**: HuggingFace's comprehensive benchmarking platform for evaluating medical large language models across diverse clinical tasks and domains.

- **[OpenMedicine](https://github.com/RamosFBC/openmedicine)**: Clinical reasoning framework emphasizing safety through DOI-traceable citations, ensuring medical claims are grounded in peer-reviewed literature.

